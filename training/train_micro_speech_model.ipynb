{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pO4-CY_TCZZS"
   },
   "source": [
    "# Train a Simple Audio Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BaFfr7DHRmGF"
   },
   "source": [
    "This notebook demonstrates how to train a 20 kB [Simple Audio Recognition](https://www.tensorflow.org/tutorials/sequences/audio_recognition) model to recognize keywords in speech.\n",
    "\n",
    "The model created in this notebook is used in the [micro_speech](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech) example for [TensorFlow Lite for MicroControllers](https://www.tensorflow.org/lite/microcontrollers/overview).\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/train/train_micro_speech_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/train/train_micro_speech_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XaVtYN4nlCft"
   },
   "source": [
    "**Training is much faster using GPU acceleration.** Before you proceed, ensure you are using a GPU runtime by going to **Runtime -> Change runtime type** and set **Hardware accelerator: GPU**. Training 15,000 iterations will take 1.5 - 2 hours on a GPU runtime.\n",
    "\n",
    "## Configure Defaults\n",
    "\n",
    "**MODIFY** the following constants for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete log file\n",
    "!rm -f jupyter_train_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redirect output to file\n",
    "import sys\n",
    "sys.stdout = open(\"jupyter_train_log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/${owner}/${repo}/archive/${hash}.tar.gz\n",
    "TF_URL = \"https://github.com/tensorflow/tensorflow/archive/v1.15.5.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download speech commands dataset\n",
    "if False:\n",
    "    import os\n",
    "    import wget\n",
    "    import tarfile\n",
    "    DATASET_DIR = \"speech_dataset\"\n",
    "    SPEECH_DATASET_URL = \"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
    "    SPEECH_DATASET_ARCHIVE = \"speech_commands_v0.02.tar.gz\"\n",
    "    SPEECH_DATASET_DIR = os.path.join(DATASET_DIR, \"speech_commands_v0.02\")\n",
    "\n",
    "    if not os.path.exists(DATASET_DIR):\n",
    "        os.mkdir(DATASET_DIR)\n",
    "    if not os.path.exists(os.path.join(DATASET_DIR, SPEECH_DATASET_ARCHIVE)):\n",
    "        wget.download(SPEECH_DATASET_URL, DATASET_DIR)\n",
    "    if not os.path.exists(SPEECH_DATASET_DIR):\n",
    "        tar_file = tarfile.open(os.path.join(DATASET_DIR, SPEECH_DATASET_ARCHIVE))\n",
    "        tar_file.extractall(SPEECH_DATASET_DIR)\n",
    "        tar_file.close()\n",
    "# os.symlink(SPEECH_DATASET_DIR, \"tf_train_speech_commands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ludfxbNIaegy"
   },
   "outputs": [],
   "source": [
    "# Training files from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands\n",
    "# copied to tf_train_speech_commands\n",
    "SPEECH_EXAMPLE_PATH = \"tf_train_speech_commands\"\n",
    "import sys\n",
    "sys.path.append(SPEECH_EXAMPLE_PATH)\n",
    "\n",
    "# A comma-delimited list of the words you want to train for.\n",
    "# The options are: yes,no,up,down,left,right,on,off,stop,go\n",
    "# All the other words will be used to train an \"unknown\" label and silent\n",
    "# audio data with no spoken words will be used to train a \"silence\" label.\n",
    "#WANTED_WORDS = \"yes,no\"\n",
    "#WANTED_WORDS = \"yes,no,up,down,left,right,on,off,stop,go\"\n",
    "#WANTED_WORDS = \"yes,no,up,down,left,right,on,off\"\n",
    "#WANTED_WORDS = \"marvin\"\n",
    "WANTED_WORDS = \"yes,no,up,down,left,right,on,off,marvin\"\n",
    "\n",
    "# The number of steps and learning rates can be specified as comma-separated\n",
    "# lists to define the rate at each stage. For example,\n",
    "# TRAINING_STEPS=12000,3000 and LEARNING_RATE=0.001,0.0001\n",
    "# will run 12,000 training loops in total, with a rate of 0.001 for the first\n",
    "# 8,000, and 0.0001 for the final 3,000.\n",
    "TRAINING_STEPS = \"12000,3000\"\n",
    "#TRAINING_STEPS = \"120,30\"\n",
    "LEARNING_RATE = \"0.001,0.0001\"\n",
    "\n",
    "# Calculate the total number of steps, which is used to identify the checkpoint\n",
    "# file name.\n",
    "TOTAL_STEPS = str(sum(map(lambda string: int(string), TRAINING_STEPS.split(\",\"))))\n",
    "\n",
    "# Print the configuration to confirm it\n",
    "print(\"Training these words: %s\" % WANTED_WORDS)\n",
    "print(\"Training steps in each stage: %s\" % TRAINING_STEPS)\n",
    "print(\"Learning rate in each stage: %s\" % LEARNING_RATE)\n",
    "print(\"Total number of training steps: %s\" % TOTAL_STEPS)\n",
    "\n",
    "# How long each spectrogram timeslice is.\n",
    "#WINDOW_SIZE_MS = 30 # default: 30\n",
    "# How loud the background noise should be, between 0 and 1.\n",
    "#TRAIN_BACKGROUND_VOLUME_RANGE = 0.1 # default: 0.1\n",
    "# How many of the training samples have background noise mixed in.\n",
    "#TRAIN_BACKGROUND_FREQUENCY = 0.8 # default: 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCgeOpvY9pAi"
   },
   "source": [
    "**DO NOT MODIFY** the following constants as they include filepaths used in this notebook and data that is shared during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nd1iM1o2ymvA"
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of 'silence' and 'unknown' training samples required\n",
    "# to ensure that we have equal number of samples for each label.\n",
    "number_of_labels = WANTED_WORDS.count(',') + 1\n",
    "number_of_total_labels = number_of_labels + 2 # for 'silence' and 'unknown' label\n",
    "equal_percentage_of_training_samples = int(100.0/(number_of_total_labels))\n",
    "SILENT_PERCENTAGE = equal_percentage_of_training_samples\n",
    "UNKNOWN_PERCENTAGE = equal_percentage_of_training_samples\n",
    "\n",
    "# Constants which are shared during training and inference\n",
    "PREPROCESS = 'micro'\n",
    "WINDOW_STRIDE = 20\n",
    "MODEL_ARCHITECTURE = 'tiny_conv' # Other options include: single_fc, conv,\n",
    "                      # low_latency_conv, low_latency_svdf, tiny_embedding_conv\n",
    "\n",
    "# Constants used during training only\n",
    "#VERBOSITY = 'WARN'\n",
    "VERBOSITY = 'INFO'\n",
    "EVAL_STEP_INTERVAL = '1000'\n",
    "SAVE_STEP_INTERVAL = '1000'\n",
    "\n",
    "# Constants for training directories and filepaths\n",
    "DATASET_DIR =  'dataset/'\n",
    "LOGS_DIR = 'logs/'\n",
    "TRAIN_DIR = 'train/' # for training checkpoints and other files.\n",
    "\n",
    "# Constants for inference directories and filepaths\n",
    "import os\n",
    "MODELS_DIR = 'models'\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODEL_TF = os.path.join(MODELS_DIR, 'model.pb')\n",
    "MODEL_TFLITE = os.path.join(MODELS_DIR, 'model.tflite')\n",
    "FLOAT_MODEL_TFLITE = os.path.join(MODELS_DIR, 'float_model.tflite')\n",
    "#MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'model.cc')\n",
    "MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'micro_speech_model_data.cpp')\n",
    "SAVED_MODEL = os.path.join(MODELS_DIR, 'saved_model')\n",
    "\n",
    "QUANT_INPUT_MIN = 0.0\n",
    "QUANT_INPUT_MAX = 26.0\n",
    "QUANT_INPUT_RANGE = QUANT_INPUT_MAX - QUANT_INPUT_MIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rLYpvtg9P4o"
   },
   "source": [
    "## Setup Environment\n",
    "\n",
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ed_XpUrU5DvY"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9Ty5mR58E4i"
   },
   "source": [
    "**DELETE** any old data from previous runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APGx0fEh7hFF"
   },
   "outputs": [],
   "source": [
    "#!rm -rf {DATASET_DIR} {LOGS_DIR} {TRAIN_DIR} {MODELS_DIR}\n",
    "!rm -rf {LOGS_DIR} {TRAIN_DIR} {MODELS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GfEUlfFBizio"
   },
   "source": [
    "Clone the TensorFlow Github Repository, which contains the relevant code required to run this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZArmzT85SLq"
   },
   "outputs": [],
   "source": [
    "#!git clone -q --depth 1 https://github.com/tensorflow/tensorflow\n",
    "#!git clone --quiet --depth 1 --branch v1.15.5 https://github.com/tensorflow/tensorflow\n",
    "#!rm -rf tensorflow\n",
    "if not os.path.exists(\"tensorflow\"):\n",
    "    !git clone --quiet --depth 1 --branch v2.13.0 https://github.com/tensorflow/tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nS9swHLSi7Bi"
   },
   "source": [
    "Load TensorBoard to visualize the accuracy and loss as training proceeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q4qF1VxP3UE4"
   },
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir {LOGS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x1J96Ron-O4R"
   },
   "source": [
    "## Training\n",
    "\n",
    "The following script downloads the dataset and begin training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJsEZx6lynbY",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 09:39:40.959907: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2023-09-04 09:39:40.995472: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3199920000 Hz\n",
      "2023-09-04 09:39:41.000835: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562fa20e78c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-04 09:39:41.000877: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "W0904 09:39:41.863051 140684725184320 deprecation.py:323] From /data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "I0904 09:39:41.968094 140684725184320 train.py:207] Training from step: 1 \n",
      "I0904 09:44:40.479246 140684725184320 train.py:258] Step #1000: rate 0.001000, accuracy 50.0%, cross entropy 1.518852\n",
      "I0904 09:44:50.735132 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[259   1   6   4   6   2   7   1   0   0   0]\n",
      " [  2  61  13  29  34  30   7  50  49   9   2]\n",
      " [  4   2 320   3  21   0  29   7   1  10   0]\n",
      " [  6   7   9 280  40  32  10   9   9   4   0]\n",
      " [  2  10   5  15 267   5  22   9   5  10   0]\n",
      " [  3  10   4  43   6 289   2   9   9   2   0]\n",
      " [  6   5  65   5  36   1 209  14   2   9   0]\n",
      " [  5   4   1   4  21   0  15 292  13   5   3]\n",
      " [  9   6   4   2  24   8   4  25 270   6   5]\n",
      " [  4   9  23  21  72   6  25  12  19 182   0]\n",
      " [  5   9   0   7   8   0   2  35  52   2  75]]\n",
      "I0904 09:44:50.737447 140684725184320 train.py:284] Step 1000: Validation accuracy = 66.8% (N=3748)\n",
      "I0904 09:44:50.738315 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-1000\"\n",
      "I0904 09:49:53.423830 140684725184320 train.py:258] Step #2000: rate 0.001000, accuracy 59.0%, cross entropy 1.263007\n",
      "I0904 09:50:03.187232 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[256   3   5   5   2   6   2   6   1   0   0]\n",
      " [  1  74  12  35  21  35   6  52  31  14   5]\n",
      " [  2   0 334   3  19   4  23   3   0   9   0]\n",
      " [  5   2   6 293  25  45  10   3   8   6   3]\n",
      " [  1  11   4  13 269   7  16  10   3  14   2]\n",
      " [  2   5   1  26   5 327   3   4   1   2   1]\n",
      " [  1   4  64   3  21   5 225  14   0  15   0]\n",
      " [  2   5   0   2   8   1  18 315   6   4   2]\n",
      " [  5   8   2   3  16  12   4  27 270   8   8]\n",
      " [  4  11  17  18  44   5  28  10  15 220   1]\n",
      " [  2  14   1   3   7   2   3  18  26   4 115]]\n",
      "I0904 09:50:03.189330 140684725184320 train.py:284] Step 2000: Validation accuracy = 72.0% (N=3748)\n",
      "I0904 09:50:03.190057 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-2000\"\n",
      "I0904 09:54:06.156816 140684725184320 train.py:258] Step #3000: rate 0.001000, accuracy 60.0%, cross entropy 1.351308\n",
      "I0904 09:54:10.991564 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[265   7   4   2   2   2   2   0   2   0   0]\n",
      " [  1 138   6  26  15  24   6  29  21  10  10]\n",
      " [  2   7 321   4  12   2  32   3   0  14   0]\n",
      " [  4  34   4 279  20  41  10   3   4   4   3]\n",
      " [  0  28   1   4 265   5  15   8   1  21   2]\n",
      " [  1  25   0  23   4 315   2   3   0   4   0]\n",
      " [  1  17  44   2  17   3 238  10   0  20   0]\n",
      " [  2  14   0   1   9   1  18 307   6   3   2]\n",
      " [  3  37   1   2  13   8   3  14 252  15  15]\n",
      " [  1  23  13  12  37   6  16  12  13 239   1]\n",
      " [  2  27   1   1   5   1   4   4  13   5 132]]\n",
      "I0904 09:54:10.992597 140684725184320 train.py:284] Step 3000: Validation accuracy = 73.4% (N=3748)\n",
      "I0904 09:54:10.993111 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-3000\"\n",
      "I0904 09:57:23.495630 140684725184320 train.py:258] Step #4000: rate 0.001000, accuracy 64.0%, cross entropy 1.037230\n",
      "I0904 09:57:32.266183 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[267   5   3   2   2   2   2   0   0   2   1]\n",
      " [  2 111   8  31  20  27   8  37  25   8   9]\n",
      " [  2   3 346   1  13   2  22   2   0   6   0]\n",
      " [  6   7  10 300  20  41   8   2   6   3   3]\n",
      " [  0  15   3   2 293   5  10   8   1  12   1]\n",
      " [  2  11   3  25   7 322   4   2   0   1   0]\n",
      " [  1  10  51   0  18   3 246  11   1   9   2]\n",
      " [  2   7   0   1   8   0  15 318   9   2   1]\n",
      " [  5  15   2   1  14  10   4  21 272   9  10]\n",
      " [  2  14  15  13  44   5  24  10  18 226   2]\n",
      " [  3  12   2   1   8   1   3   6  16   3 140]]\n",
      "I0904 09:57:32.267324 140684725184320 train.py:284] Step 4000: Validation accuracy = 75.8% (N=3748)\n",
      "I0904 09:57:32.267988 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-4000\"\n",
      "I0904 10:02:11.974869 140684725184320 train.py:258] Step #5000: rate 0.001000, accuracy 70.0%, cross entropy 0.928894\n",
      "I0904 10:02:21.468020 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[268   2   5   2   5   1   1   2   0   0   0]\n",
      " [  1 109   5  32  22  37   4  26  28  12  10]\n",
      " [  1   4 352   2  14   2  15   1   0   5   1]\n",
      " [  5   4   9 299  22  46   5   1   7   5   3]\n",
      " [  0  12   4   5 293   6   4   5   2  18   1]\n",
      " [  1   8   3  24   5 331   3   0   1   1   0]\n",
      " [  1  11  58   2  17   6 230   9   1  17   0]\n",
      " [  2   5   0   1   7   1  13 316  10   7   1]\n",
      " [  3  10   3   0  13  11   2  15 287  13   6]\n",
      " [  2   7  17  13  38   9  11   7  21 247   1]\n",
      " [  2  16   1   1   6   4   1   4  16   5 139]]\n",
      "I0904 10:02:21.470182 140684725184320 train.py:284] Step 5000: Validation accuracy = 76.6% (N=3748)\n",
      "I0904 10:02:21.471139 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-5000\"\n",
      "I0904 10:06:18.728282 140684725184320 train.py:258] Step #6000: rate 0.001000, accuracy 81.0%, cross entropy 0.726960\n",
      "I0904 10:06:28.536301 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[274   1   1   0   2   3   3   1   1   0   0]\n",
      " [  2 100   5  34  32  26   5  30  26  16  10]\n",
      " [  1   1 341   2  14   2  28   0   0   8   0]\n",
      " [  7   3   5 308  24  35   6   1   7   8   2]\n",
      " [  0   9   0   2 305   5   9   4   2  13   1]\n",
      " [  1   9   1  27   6 326   5   0   1   1   0]\n",
      " [  1   7  32   1  19   1 263   9   1  17   1]\n",
      " [  2   4   0   1   7   1  18 317   8   4   1]\n",
      " [  5   8   2   0  15   8   2  14 289  14   6]\n",
      " [  4   9   8  13  44   6  17   5  18 249   0]\n",
      " [  3  14   1   0   7   3   3   4  14   4 142]]\n",
      "I0904 10:06:28.537724 140684725184320 train.py:284] Step 6000: Validation accuracy = 77.7% (N=3748)\n",
      "I0904 10:06:28.538277 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-6000\"\n",
      "W0904 10:06:28.547494 140684725184320 deprecation.py:323] From /data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "I0904 10:11:00.692164 140684725184320 train.py:258] Step #7000: rate 0.001000, accuracy 83.0%, cross entropy 0.648333\n",
      "I0904 10:11:10.255668 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[267   0   2   0   5   7   2   2   0   0   1]\n",
      " [  1 109   4  28  25  37   6  24  29  13  10]\n",
      " [  1   2 353   2   9   0  25   0   0   5   0]\n",
      " [  4   7   9 303  19  43   5   1   6   6   3]\n",
      " [  0  10   2   3 298   5  12   4   1  14   1]\n",
      " [  1   8   1  24   5 332   4   0   1   1   0]\n",
      " [  1   9  46   1  11   1 266   6   0  10   1]\n",
      " [  2   5   1   1   6   1  16 317  10   3   1]\n",
      " [  4   7   3   1  11  11   7  13 288  12   6]\n",
      " [  2  13  16  12  36   7  20   5  13 248   1]\n",
      " [  2  15   1   1   6   4   3   2   9   4 148]]\n",
      "I0904 10:11:10.257085 140684725184320 train.py:284] Step 7000: Validation accuracy = 78.1% (N=3748)\n",
      "I0904 10:11:10.257889 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-7000\"\n",
      "I0904 10:16:04.917800 140684725184320 train.py:258] Step #8000: rate 0.001000, accuracy 75.0%, cross entropy 0.890836\n",
      "I0904 10:16:14.542039 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[274   2   0   2   5   0   0   2   0   1   0]\n",
      " [  2 107   4  18  33  40   3  22  32  13  12]\n",
      " [  2   3 358   2  12   1  15   0   0   4   0]\n",
      " [  7   5   7 281  29  54   4   1   8   7   3]\n",
      " [  0   8   0   1 320   3   3   4   1   9   1]\n",
      " [  1   8   1  15   6 339   4   0   1   2   0]\n",
      " [  1   7  49   0  21   1 252   8   0  12   1]\n",
      " [  2   3   0   1   9   0  14 318  11   4   1]\n",
      " [  6   9   3   0  16   7   0  13 293  10   6]\n",
      " [  4   8   9   6  50   4   9   2  19 262   0]\n",
      " [  4   7   2   0   7   3   0   2  12   5 153]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0904 10:16:14.543445 140684725184320 train.py:284] Step 8000: Validation accuracy = 78.9% (N=3748)\n",
      "I0904 10:16:14.544268 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-8000\"\n",
      "I0904 10:21:05.117370 140684725184320 train.py:258] Step #9000: rate 0.001000, accuracy 68.0%, cross entropy 1.011075\n",
      "I0904 10:21:14.613492 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[273   1   0   2   2   2   3   3   0   0   0]\n",
      " [  1 125   4  20  21  41   4  23  22  12  13]\n",
      " [  2   5 364   3   8   0  12   0   0   3   0]\n",
      " [  7  12   6 283  18  58   5   1   8   5   3]\n",
      " [  0  17   0   1 304   5   7   4   1  10   1]\n",
      " [  2  11   0  15   5 338   4   0   1   1   0]\n",
      " [  1  13  42   0  12   1 260  10   1  11   1]\n",
      " [  2   6   0   1   5   1  14 320  10   3   1]\n",
      " [  5  11   2   0  13   9   0  13 293   9   8]\n",
      " [  4  17  10   8  32   5  13   5  15 264   0]\n",
      " [  3  15   1   1   6   4   1   2   9   3 150]]\n",
      "I0904 10:21:14.615043 140684725184320 train.py:284] Step 9000: Validation accuracy = 79.3% (N=3748)\n",
      "I0904 10:21:14.616191 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-9000\"\n",
      "I0904 10:26:02.526359 140684725184320 train.py:258] Step #10000: rate 0.001000, accuracy 78.0%, cross entropy 0.743681\n",
      "I0904 10:26:12.108922 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[273   3   1   0   4   2   2   0   0   1   0]\n",
      " [  1 122   7  28  24  26   5  24  32   9   8]\n",
      " [  1   3 358   3   7   1  19   0   0   5   0]\n",
      " [  3   8   7 312  17  33   7   2   9   5   3]\n",
      " [  0   9   0   2 307   4  12   5   1   9   1]\n",
      " [  2  11   1  26   6 325   4   0   1   1   0]\n",
      " [  1   9  36   1  11   1 275   9   0   8   1]\n",
      " [  2   4   0   1   5   1  15 321  10   4   0]\n",
      " [  5  11   3   0  11   4   1  13 305   7   3]\n",
      " [  2  15   7   8  34   3  16   4  20 263   1]\n",
      " [  3  17   2   1   6   1   2   3  12   3 145]]\n",
      "I0904 10:26:12.110908 140684725184320 train.py:284] Step 10000: Validation accuracy = 80.2% (N=3748)\n",
      "I0904 10:26:12.111745 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-10000\"\n",
      "I0904 10:31:02.833023 140684725184320 train.py:258] Step #11000: rate 0.001000, accuracy 68.0%, cross entropy 0.922443\n",
      "I0904 10:31:13.255910 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[278   4   0   0   1   2   1   0   0   0   0]\n",
      " [  1 119   7  31  30  22   7  29  23   5  12]\n",
      " [  1   3 355   3   7   0  25   0   0   3   0]\n",
      " [  3   8   7 312  24  30   9   2   5   4   2]\n",
      " [  0  12   0   2 311   4  12   3   1   4   1]\n",
      " [  2  11   1  28   5 323   5   0   1   1   0]\n",
      " [  1   7  27   2  12   0 292   5   0   5   1]\n",
      " [  2   6   0   1   6   0  14 325   5   3   1]\n",
      " [  4  15   1   0  12   6   3  16 290   9   7]\n",
      " [  2  15   8  10  35   3  23   6  20 251   0]\n",
      " [  3  16   2   1   6   1   2   3   5   3 153]]\n",
      "I0904 10:31:13.257564 140684725184320 train.py:284] Step 11000: Validation accuracy = 80.3% (N=3748)\n",
      "I0904 10:31:13.258510 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-11000\"\n",
      "I0904 10:36:00.214631 140684725184320 train.py:258] Step #12000: rate 0.001000, accuracy 68.0%, cross entropy 0.980020\n",
      "I0904 10:36:09.046303 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[272   1   1   1   6   2   2   1   0   0   0]\n",
      " [  1 117   5  27  19  30   4  24  29  13  17]\n",
      " [  1   4 360   3   7   1  15   0   0   5   1]\n",
      " [  4  11   5 308  17  36   7   2   6   5   5]\n",
      " [  0  10   0   1 316   3   6   4   1   8   1]\n",
      " [  2  11   1  25   6 326   4   0   1   1   0]\n",
      " [  1   6  34   3  13   0 275  12   0   7   1]\n",
      " [  2   4   0   1   6   1  13 323   9   3   1]\n",
      " [  5   9   1   0  14   7   1  13 294  10   9]\n",
      " [  2  13   6   6  35   3  13   3  18 271   3]\n",
      " [  3  12   2   2   6   1   2   2   9   3 153]]\n",
      "I0904 10:36:09.047707 140684725184320 train.py:284] Step 12000: Validation accuracy = 80.4% (N=3748)\n",
      "I0904 10:36:09.048934 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-12000\"\n",
      "I0904 10:40:57.618518 140684725184320 train.py:258] Step #13000: rate 0.000100, accuracy 77.0%, cross entropy 0.827451\n",
      "I0904 10:41:07.112922 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[280   0   0   2   3   0   1   0   0   0   0]\n",
      " [  1 120   6  21  20  38   3  20  29  14  14]\n",
      " [  1   7 361   2   6   1  12   0   0   6   1]\n",
      " [  4  14   5 299  16  44   7   2   7   5   3]\n",
      " [  0  12   0   1 315   3   6   3   1   9   0]\n",
      " [  2  10   1  19   6 334   4   0   1   0   0]\n",
      " [  1  11  34   2  11   1 274   8   0   9   1]\n",
      " [  2   4   0   1   6   1  15 318  12   3   1]\n",
      " [  5  11   1   0  12   7   1  11 300   9   6]\n",
      " [  2  13   6   6  33   4  12   2  18 276   1]\n",
      " [  3  17   1   1   5   1   1   2  10   4 150]]\n",
      "I0904 10:41:07.115220 140684725184320 train.py:284] Step 13000: Validation accuracy = 80.8% (N=3748)\n",
      "I0904 10:41:07.115902 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-13000\"\n",
      "I0904 10:46:00.801578 140684725184320 train.py:258] Step #14000: rate 0.000100, accuracy 75.0%, cross entropy 0.896356\n",
      "I0904 10:46:10.828651 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[268   5   0   1   4   4   2   0   1   1   0]\n",
      " [  1 115   7  27  19  31   4  23  30  15  14]\n",
      " [  1   3 362   2   5   1  17   0   0   6   0]\n",
      " [  4   9   4 312  17  34   7   2   8   6   3]\n",
      " [  0  10   0   1 314   3   8   3   1  10   0]\n",
      " [  2   9   1  23   6 330   4   0   1   1   0]\n",
      " [  1   4  34   1  12   1 281   8   0   9   1]\n",
      " [  2   4   0   1   6   1  15 320  10   3   1]\n",
      " [  4   8   1   0  14   7   1  11 299  13   5]\n",
      " [  2  11   5   6  30   4  16   2  16 280   1]\n",
      " [  3  15   2   1   5   1   2   2  10   4 150]]\n",
      "I0904 10:46:10.830412 140684725184320 train.py:284] Step 14000: Validation accuracy = 80.9% (N=3748)\n",
      "I0904 10:46:10.831440 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-14000\"\n",
      "I0904 10:51:10.895574 140684725184320 train.py:258] Step #15000: rate 0.000100, accuracy 73.0%, cross entropy 0.903169\n",
      "I0904 10:51:20.254071 140684725184320 train.py:282] Confusion Matrix:\n",
      " [[274   2   0   0   3   4   0   2   0   1   0]\n",
      " [  1 110   7  26  25  33   4  24  30  12  14]\n",
      " [  1   2 365   2   5   0  17   0   0   5   0]\n",
      " [  4   5   4 317  19  32   7   2   8   5   3]\n",
      " [  0   9   0   1 318   3   6   4   1   8   0]\n",
      " [  2   9   1  23   6 330   4   0   1   1   0]\n",
      " [  1   4  31   3  13   1 280  10   0   8   1]\n",
      " [  2   4   0   1   6   1  15 320  10   3   1]\n",
      " [  4   8   1   0  14   7   0  14 299  12   4]\n",
      " [  2  12   5   6  36   4  17   2  17 271   1]\n",
      " [  3  14   1   2   5   1   2   3   9   4 151]]\n",
      "I0904 10:51:20.256441 140684725184320 train.py:284] Step 15000: Validation accuracy = 81.0% (N=3748)\n",
      "I0904 10:51:20.257321 140684725184320 train.py:292] Saving to \"train/tiny_conv.ckpt-15000\"\n",
      "I0904 10:51:20.279566 140684725184320 train.py:296] set_size=4080\n",
      "W0904 10:51:30.897274 140684725184320 train.py:315] Confusion Matrix:\n",
      " [[302   1   0   1   6   0   0   0   0   2   0]\n",
      " [  0 109   5  29  31  43  12  26  39  11   7]\n",
      " [  3   1 371   2   7   5  19   2   0   8   1]\n",
      " [  1  25   3 310  13  31   7   1   2   4   8]\n",
      " [  2   8   1   1 362   8  11   4   6  21   1]\n",
      " [  4  11   2  34  15 327   4   0   6   3   0]\n",
      " [  1  14  26   0  21   5 314   7   1  20   3]\n",
      " [  2  23   1   0   6   2  12 334  15   1   0]\n",
      " [  3  14   0   4   7  22   2   8 318  16   2]\n",
      " [  1  10   3   6  19   3  11   3  10 334   2]\n",
      " [  0   9   3   1   3   1   0   4  12   4 158]]\n",
      "W0904 10:51:30.898934 140684725184320 train.py:317] Final test accuracy = 79.4% (N=4080)\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/train.py\n",
    "# https://github.com/tensorflow/tensorflow/tree/r1.15/tensorflow/examples/speech_commands/train.py\n",
    "#!python tensorflow/tensorflow/examples/speech_commands/train.py \\\n",
    "#!python ./tf_train_speech_commands/train.py \\\n",
    "#%run tf_train_speech_commands/train.py \\\n",
    "%run tensorflow/tensorflow/examples/speech_commands/train.py \\\n",
    "#--data_url=  \\\n",
    "--data_dir={DATASET_DIR} \\\n",
    "--wanted_words={WANTED_WORDS} \\\n",
    "--silence_percentage={SILENT_PERCENTAGE} \\\n",
    "--unknown_percentage={UNKNOWN_PERCENTAGE} \\\n",
    "--preprocess={PREPROCESS} \\\n",
    "--window_stride={WINDOW_STRIDE} \\\n",
    "--model_architecture={MODEL_ARCHITECTURE} \\\n",
    "--how_many_training_steps={TRAINING_STEPS} \\\n",
    "--learning_rate={LEARNING_RATE} \\\n",
    "--train_dir={TRAIN_DIR} \\\n",
    "--summaries_dir={LOGS_DIR} \\\n",
    "--verbosity={VERBOSITY} \\\n",
    "--eval_step_interval={EVAL_STEP_INTERVAL} \\\n",
    "--save_step_interval={SAVE_STEP_INTERVAL}\n",
    "#--window_size_ms={WINDOW_SIZE_MS} \\\n",
    "#--background_volume={TRAIN_BACKGROUND_VOLUME_RANGE} \\\n",
    "#--background_frequency={TRAIN_BACKGROUND_FREQUENCY}\n",
    "# added: window_size_ms background_volume background_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UczQKtqLi7OJ"
   },
   "source": [
    "# Skipping the training\n",
    "\n",
    "If you don't want to spend an hour or two training the model from scratch, you can download pretrained checkpoints by uncommenting the lines below (removing the '#'s at the start of each line) and running them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZw3VNlnla-J"
   },
   "outputs": [],
   "source": [
    "#!curl -O \"https://storage.googleapis.com/download.tensorflow.org/models/tflite/speech_micro_train_2020_05_10.tgz\"\n",
    "#!tar xzf speech_micro_train_2020_05_10.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQUJLrdS-ftl"
   },
   "source": [
    "## Generate a TensorFlow Model for Inference\n",
    "\n",
    "Combine relevant training results (graph, weights, etc) into a single file for inference. This process is known as freezing a model and the resulting model is known as a frozen model/graph, as it cannot be further re-trained after this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redirect output to file\n",
    "!rm -f jupyter_freeze_log.txt\n",
    "import sys\n",
    "sys.stdout = open(\"jupyter_freeze_log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xyc3_eLh9sAg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "I0904 10:51:31.362257 140684725184320 saver.py:1284] Restoring parameters from train/tiny_conv.ckpt-15000\n",
      "W0904 10:51:31.368982 140684725184320 deprecation.py:323] From /data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/tensorflow/tensorflow/examples/speech_commands/freeze.py:230: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "W0904 10:51:31.369951 140684725184320 deprecation.py:323] From /data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "I0904 10:51:31.372818 140684725184320 graph_util_impl.py:334] Froze 4 variables.\n",
      "I0904 10:51:31.374619 140684725184320 graph_util_impl.py:394] Converted 4 variables to const ops.\n",
      "W0904 10:51:31.375907 140684725184320 deprecation.py:323] From /data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/tensorflow/tensorflow/examples/speech_commands/freeze.py:183: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "I0904 10:51:31.376893 140684725184320 builder_impl.py:640] No assets to save.\n",
      "I0904 10:51:31.377631 140684725184320 builder_impl.py:460] No assets to write.\n",
      "I0904 10:51:31.400376 140684725184320 builder_impl.py:425] SavedModel written to: models/saved_model/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "!rm -rf {SAVED_MODEL}\n",
    "#!python tensorflow/tensorflow/examples/speech_commands/freeze.py \\\n",
    "#!python tf_train_speech_commands/freeze.py \\\n",
    "%run tensorflow/tensorflow/examples/speech_commands/freeze.py \\\n",
    "--wanted_words=$WANTED_WORDS \\\n",
    "--window_stride_ms=$WINDOW_STRIDE \\\n",
    "--preprocess=$PREPROCESS \\\n",
    "--model_architecture=$MODEL_ARCHITECTURE \\\n",
    "--start_checkpoint=$TRAIN_DIR$MODEL_ARCHITECTURE'.ckpt-'{TOTAL_STEPS} \\\n",
    "--save_format=saved_model \\\n",
    "--output_file={SAVED_MODEL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DBGDxVI-nKG"
   },
   "source": [
    "## Generate a TensorFlow Lite Model\n",
    "\n",
    "Convert the frozen graph into a TensorFlow Lite model, which is fully quantized for use with embedded devices.\n",
    "\n",
    "The following cell will also print the model size, which will be under 20 kilobytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redirect output to file\n",
    "!rm -f jupyter_model_log.txt\n",
    "import sys\n",
    "sys.stdout = open(\"jupyter_model_log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIitkqvGWmre"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# We add this path so we can import the speech processing modules.\n",
    "#sys.path.append(\"/content/tensorflow/tensorflow/examples/speech_commands/\")\n",
    "import input_data\n",
    "import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzqECqMxgBh4"
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "CLIP_DURATION_MS = 1000\n",
    "WINDOW_SIZE_MS = 30.0\n",
    "FEATURE_BIN_COUNT = 40\n",
    "BACKGROUND_FREQUENCY = 0.8\n",
    "BACKGROUND_VOLUME_RANGE = 0.1\n",
    "#BACKGROUND_FREQUENCY = TRAIN_BACKGROUND_FREQUENCY\n",
    "#BACKGROUND_VOLUME_RANGE = TRAIN_BACKGROUND_VOLUME_RANGE\n",
    "TIME_SHIFT_MS = 100.0\n",
    "\n",
    "#DATA_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
    "DATA_URL=''\n",
    "VALIDATION_PERCENTAGE = 10\n",
    "TESTING_PERCENTAGE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNQdAplJV1fz"
   },
   "outputs": [],
   "source": [
    "model_settings = models.prepare_model_settings(\n",
    "    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),\n",
    "    SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
    "    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
    "audio_processor = input_data.AudioProcessor(\n",
    "    DATA_URL, DATASET_DIR,\n",
    "    SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,\n",
    "    WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,\n",
    "    TESTING_PERCENTAGE, model_settings, LOGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBj_AyCh1cC0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0904 10:51:32.731474 140684725184320 deprecation.py:323] From /data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/lite/python/convert_saved_model.py:60: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "I0904 10:51:32.742612 140684725184320 saver.py:1284] Restoring parameters from models/saved_model/variables/variables\n",
      "I0904 10:51:32.749812 140684725184320 convert_saved_model.py:80] The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\n",
      "I0904 10:51:32.752012 140684725184320 convert_saved_model.py:99] input tensors info: \n",
      "I0904 10:51:32.752825 140684725184320 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: input\n",
      "I0904 10:51:32.753529 140684725184320 convert_saved_model.py:43]  tensor name: Reshape_1:0, shape: (1, 1960), type: DT_FLOAT\n",
      "I0904 10:51:32.754123 140684725184320 convert_saved_model.py:101] output tensors info: \n",
      "I0904 10:51:32.754838 140684725184320 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: output\n",
      "I0904 10:51:32.756430 140684725184320 convert_saved_model.py:43]  tensor name: labels_softmax:0, shape: (1, 11), type: DT_FLOAT\n",
      "I0904 10:51:32.766549 140684725184320 saver.py:1284] Restoring parameters from models/saved_model/variables/variables\n",
      "2023-09-04 10:51:32.774638: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\n",
      "2023-09-04 10:51:32.774762: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2023-09-04 10:51:32.779908: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\n",
      "2023-09-04 10:51:32.779924: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "2023-09-04 10:51:32.779928: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "I0904 10:51:32.788401 140684725184320 graph_util_impl.py:334] Froze 4 variables.\n",
      "I0904 10:51:32.790031 140684725184320 graph_util_impl.py:394] Converted 4 variables to const ops.\n",
      "2023-09-04 10:51:32.792218: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\n",
      "2023-09-04 10:51:32.792304: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2023-09-04 10:51:32.795381: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node first_weights/Assign doesn't exist in graph\n",
      "I0904 10:51:33.691210 140684725184320 saver.py:1284] Restoring parameters from models/saved_model/variables/variables\n",
      "I0904 10:51:33.699709 140684725184320 convert_saved_model.py:80] The given SavedModel MetaGraphDef contains SignatureDefs with the following keys: {'serving_default'}\n",
      "I0904 10:51:33.700406 140684725184320 convert_saved_model.py:99] input tensors info: \n",
      "I0904 10:51:33.701070 140684725184320 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: input\n",
      "I0904 10:51:33.701601 140684725184320 convert_saved_model.py:43]  tensor name: Reshape_1:0, shape: (1, 1960), type: DT_FLOAT\n",
      "I0904 10:51:33.702021 140684725184320 convert_saved_model.py:101] output tensors info: \n",
      "I0904 10:51:33.702718 140684725184320 convert_saved_model.py:41] Tensor's key in saved_model's tensor_map: output\n",
      "I0904 10:51:33.703092 140684725184320 convert_saved_model.py:43]  tensor name: labels_softmax:0, shape: (1, 11), type: DT_FLOAT\n",
      "I0904 10:51:33.712495 140684725184320 saver.py:1284] Restoring parameters from models/saved_model/variables/variables\n",
      "2023-09-04 10:51:33.721341: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\n",
      "2023-09-04 10:51:33.721479: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2023-09-04 10:51:33.726468: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\n",
      "2023-09-04 10:51:33.726483: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "2023-09-04 10:51:33.726486: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "I0904 10:51:33.735173 140684725184320 graph_util_impl.py:334] Froze 4 variables.\n",
      "I0904 10:51:33.736700 140684725184320 graph_util_impl.py:394] Converted 4 variables to const ops.\n",
      "W0904 10:51:33.738113 140684725184320 module_wrapper.py:139] From /tmp/ipykernel_636058/2607024760.py:11: The name tf.lite.constants.INT8 is deprecated. Please use tf.compat.v1.lite.constants.INT8 instead.\n",
      "\n",
      "2023-09-04 10:51:33.739227: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\n",
      "2023-09-04 10:51:33.739520: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2023-09-04 10:51:33.742918: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node first_weights/Assign doesn't exist in graph\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow 1\n",
    "#with tf.Session() as sess:\n",
    "with tf.compat.v1.Session() as sess:\n",
    "  float_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
    "  float_tflite_model = float_converter.convert()\n",
    "  float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
    "  print(\"Float model is %d bytes\" % float_tflite_model_size)\n",
    "\n",
    "  converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  converter.inference_input_type = tf.lite.constants.INT8\n",
    "  converter.inference_output_type = tf.lite.constants.INT8\n",
    "  def representative_dataset_gen():\n",
    "    for i in range(100):\n",
    "      data, _ = audio_processor.get_data(1, i*1, model_settings,\n",
    "                                         BACKGROUND_FREQUENCY, \n",
    "                                         BACKGROUND_VOLUME_RANGE,\n",
    "                                         TIME_SHIFT_MS,\n",
    "                                         'testing',\n",
    "                                         sess)\n",
    "      flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(1, 1960)\n",
    "      yield [flattened_data]\n",
    "  converter.representative_dataset = representative_dataset_gen\n",
    "  tflite_model = converter.convert()\n",
    "  tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
    "  print(\"Quantized model is %d bytes\" % tflite_model_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBj_AyCh1cC0"
   },
   "outputs": [],
   "source": [
    "# Tensorflow 2\n",
    "if False:\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "    # with tf.Session() as sess:\n",
    "      float_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
    "      float_tflite_model = float_converter.convert()\n",
    "      float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
    "      print(\"Float model is %d bytes\" % float_tflite_model_size)\n",
    "\n",
    "      converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
    "      converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "      #converter.inference_input_type = tf.lite.constants.INT8\n",
    "      #converter.inference_output_type = tf.lite.constants.INT8\n",
    "\n",
    "      # TF 2\n",
    "      converter.target_spec.supported_types = [tf.int8]\n",
    "\n",
    "      def representative_dataset_gen():\n",
    "        for i in range(100):\n",
    "          data, _ = audio_processor.get_data(1, i*1, model_settings,\n",
    "                                             BACKGROUND_FREQUENCY, \n",
    "                                             BACKGROUND_VOLUME_RANGE,\n",
    "                                             TIME_SHIFT_MS,\n",
    "                                             'testing',\n",
    "                                             sess)\n",
    "          flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(1, 1960)\n",
    "          yield [flattened_data]\n",
    "      converter.representative_dataset = representative_dataset_gen\n",
    "      tflite_model = converter.convert()\n",
    "      tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
    "      print(\"Quantized model is %d bytes\" % tflite_model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EeLiDZTbLkzv"
   },
   "source": [
    "## Testing the TensorFlow Lite model's accuracy\n",
    "\n",
    "Verify that the model we've exported is still accurate, using the TF Lite Python API and our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQsEteKRLryJ"
   },
   "outputs": [],
   "source": [
    "# Tensorflow 1\n",
    "# Helper function to run inference\n",
    "def run_tflite_inference(tflite_model_path, model_type=\"Float\"):\n",
    "  # Load test data\n",
    "  np.random.seed(0) # set random seed for reproducible test results.\n",
    "\n",
    "  tf.compat.v1.disable_eager_execution()\n",
    "  with tf.compat.v1.Session() as sess:\n",
    "    \n",
    "  # with tf.Session() as sess:\n",
    "    test_data, test_labels = audio_processor.get_data(\n",
    "        -1, 0, model_settings, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n",
    "        TIME_SHIFT_MS, 'testing', sess)\n",
    "  test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  # For quantized models, manually quantize the input data from float to integer\n",
    "  if model_type == \"Quantized\":\n",
    "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "    test_data = test_data / input_scale + input_zero_point\n",
    "    test_data = test_data.astype(input_details[\"dtype\"])\n",
    "\n",
    "  correct_predictions = 0\n",
    "  for i in range(len(test_data)):\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    top_prediction = output.argmax()\n",
    "    correct_predictions += (top_prediction == test_labels[i])\n",
    "\n",
    "  print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
    "      model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQsEteKRLryJ"
   },
   "outputs": [],
   "source": [
    "# Tensorflow 2\n",
    "if False:\n",
    "    # Helper function to run inference\n",
    "    def run_tflite_inference(tflite_model_path, model_type=\"Float\"):\n",
    "      # Load test data\n",
    "      np.random.seed(0) # set random seed for reproducible test results.\n",
    "      with tf.Session() as sess:\n",
    "        test_data, test_labels = audio_processor.get_data(\n",
    "            -1, 0, model_settings, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n",
    "            TIME_SHIFT_MS, 'testing', sess)\n",
    "      test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n",
    "\n",
    "      # Initialize the interpreter\n",
    "      interpreter = tf.lite.Interpreter(tflite_model_path)\n",
    "      interpreter.allocate_tensors()\n",
    "\n",
    "      input_details = interpreter.get_input_details()[0]\n",
    "      output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "      # For quantized models, manually quantize the input data from float to integer\n",
    "      if model_type == \"Quantized\":\n",
    "        input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "        test_data = test_data / input_scale + input_zero_point\n",
    "        test_data = test_data.astype(input_details[\"dtype\"])\n",
    "\n",
    "      correct_predictions = 0\n",
    "      for i in range(len(test_data)):\n",
    "        interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "        top_prediction = output.argmax()\n",
    "        correct_predictions += (top_prediction == test_labels[i])\n",
    "\n",
    "      print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
    "          model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l-pD52Na6jRa"
   },
   "outputs": [],
   "source": [
    "# Compute float model accuracy\n",
    "run_tflite_inference(FLOAT_MODEL_TFLITE)\n",
    "\n",
    "# Compute quantized model accuracy\n",
    "run_tflite_inference(MODEL_TFLITE, model_type='Quantized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dt6Zqbxu-wIi"
   },
   "source": [
    "## Generate a TensorFlow Lite for MicroControllers Model\n",
    "Convert the TensorFlow Lite model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XohZOTjR8ZyE"
   },
   "outputs": [],
   "source": [
    "# Install xxd if it is not available\n",
    "#!apt-get update && apt-get -qq install xxd\n",
    "# Convert to a C source file\n",
    "#!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
    "# Update variable names\n",
    "#REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
    "#!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XohZOTjR8ZyE"
   },
   "outputs": [],
   "source": [
    "# Install xxd if it is not available\n",
    "#!apt-get update && apt-get -qq install xxd\n",
    "# Convert to a C source file\n",
    "#!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
    "#MICRO_MODEL_FILENAME = \"models/micro_speech_model_data.cpp\"\n",
    "#print(MODEL_TFLITE)\n",
    "#print(MODEL_TFLITE_MICRO)\n",
    "#print(MICRO_MODEL_FILENAME)\n",
    "#!echo {MODEL_TFLITE} {MICRO_MODEL_FILENAME}\n",
    "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
    "# Update variable names\n",
    "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
    "#print(REPLACE_TEXT)\n",
    "!sed -i 's/_len/_size/g' {MODEL_TFLITE_MICRO}\n",
    "!sed -i 's/unsigned/const unsigned/g' {MODEL_TFLITE_MICRO}\n",
    "#!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}\n",
    "!sed -i 's/'{REPLACE_TEXT}'/g_micro_speech_model_data/g' {MODEL_TFLITE_MICRO}\n",
    "#!sed -i '1s/^/#include <cstdint>\\n#include \"micro_speech_model_data.h\"\\nconst unsigned int g_micro_speech_model_data_size = '{tflite_model_size}';\\nalignas(16) const /g' {MODEL_TFLITE_MICRO}\n",
    "!sed -i '1s/^/#include <cstdint>\\n#include \"micro_speech_model_data.h\"\\nalignas(16) /g' {MODEL_TFLITE_MICRO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pQnN0i_-0L2"
   },
   "source": [
    "## Deploy to a Microcontroller\n",
    "\n",
    "Follow the instructions in the [micro_speech](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech) README.md for [TensorFlow Lite for MicroControllers](https://www.tensorflow.org/lite/microcontrollers/overview) to deploy this model on a specific microcontroller.\n",
    "\n",
    "**Reference Model:** If you have not modified this notebook, you can follow the instructions as is, to deploy the model. Refer to the [`micro_speech/train/models`](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/train/models) directory to access the models generated in this notebook.\n",
    "\n",
    "**New Model:** If you have generated a new model to identify different words: (i) Update `kCategoryCount` and `kCategoryLabels` in [`micro_speech/micro_features/micro_model_settings.h`](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/micro_features/micro_model_settings.h) and (ii) Update the values assigned to the variables defined in [`micro_speech/micro_features/model.cc`](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/micro_features/model.cc) with values displayed after running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoYyh0VU8pca"
   },
   "outputs": [],
   "source": [
    "# Redirect output to file\n",
    "#!rm -f micro_speech_model_data.cpp\n",
    "#import sys\n",
    "#sys.stdout = open(\"micro_speech_model_data.cpp\", \"w\")# Print the C source file\n",
    "#!cat {MODEL_TFLITE_MICRO}\n",
    "#!cat {MICRO_MODEL_FILENAME}\n",
    "#print(\"MODEL_TFLITE_MICRO: \", MODEL_TFLITE_MICRO)\n",
    "#print(\"MICRO_MODEL_FILENAME: \", MICRO_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print log\n",
    "#!cat jupyter_model_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_micro_speech_model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
