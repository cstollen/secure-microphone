{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pO4-CY_TCZZS"
   },
   "source": [
    "# Train a Simple Audio Recognition Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BaFfr7DHRmGF"
   },
   "source": [
    "This notebook demonstrates how to train a 20 kB [Simple Audio Recognition](https://www.tensorflow.org/tutorials/sequences/audio_recognition) model to recognize keywords in speech.\n",
    "\n",
    "The model created in this notebook is used in the [micro_speech](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech) example for [TensorFlow Lite for MicroControllers](https://www.tensorflow.org/lite/microcontrollers/overview).\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/train/train_micro_speech_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/train/train_micro_speech_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XaVtYN4nlCft"
   },
   "source": [
    "**Training is much faster using GPU acceleration.** Before you proceed, ensure you are using a GPU runtime by going to **Runtime -> Change runtime type** and set **Hardware accelerator: GPU**. Training 15,000 iterations will take 1.5 - 2 hours on a GPU runtime.\n",
    "\n",
    "## Configure Defaults\n",
    "\n",
    "**MODIFY** the following constants for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete log file\n",
    "!rm -f jupyter_train_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redirect output to file\n",
    "import sys\n",
    "sys.stdout = open(\"jupyter_train_log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/${owner}/${repo}/archive/${hash}.tar.gz\n",
    "TF_URL = \"https://github.com/tensorflow/tensorflow/archive/v1.15.5.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download speech commands dataset\n",
    "if False:\n",
    "    import os\n",
    "    import wget\n",
    "    import tarfile\n",
    "    DATASET_DIR = \"speech_dataset\"\n",
    "    SPEECH_DATASET_URL = \"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
    "    SPEECH_DATASET_ARCHIVE = \"speech_commands_v0.02.tar.gz\"\n",
    "    SPEECH_DATASET_DIR = os.path.join(DATASET_DIR, \"speech_commands_v0.02\")\n",
    "\n",
    "    if not os.path.exists(DATASET_DIR):\n",
    "        os.mkdir(DATASET_DIR)\n",
    "    if not os.path.exists(os.path.join(DATASET_DIR, SPEECH_DATASET_ARCHIVE)):\n",
    "        wget.download(SPEECH_DATASET_URL, DATASET_DIR)\n",
    "    if not os.path.exists(SPEECH_DATASET_DIR):\n",
    "        tar_file = tarfile.open(os.path.join(DATASET_DIR, SPEECH_DATASET_ARCHIVE))\n",
    "        tar_file.extractall(SPEECH_DATASET_DIR)\n",
    "        tar_file.close()\n",
    "# os.symlink(SPEECH_DATASET_DIR, \"tf_train_speech_commands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ludfxbNIaegy"
   },
   "outputs": [],
   "source": [
    "# Training files from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands\n",
    "# copied to tf_train_speech_commands\n",
    "SPEECH_EXAMPLE_PATH = \"tf_train_speech_commands\"\n",
    "import sys\n",
    "sys.path.append(SPEECH_EXAMPLE_PATH)\n",
    "\n",
    "# A comma-delimited list of the words you want to train for.\n",
    "# The options are: yes,no,up,down,left,right,on,off,stop,go\n",
    "# All the other words will be used to train an \"unknown\" label and silent\n",
    "# audio data with no spoken words will be used to train a \"silence\" label.\n",
    "#WANTED_WORDS = \"yes,no\"\n",
    "#WANTED_WORDS = \"yes,no,up,down,left,right,on,off,stop,go\"\n",
    "#WANTED_WORDS = \"yes,no,up,down,left,right,on,off\"\n",
    "WANTED_WORDS = \"marvin\"\n",
    "\n",
    "# The number of steps and learning rates can be specified as comma-separated\n",
    "# lists to define the rate at each stage. For example,\n",
    "# TRAINING_STEPS=12000,3000 and LEARNING_RATE=0.001,0.0001\n",
    "# will run 12,000 training loops in total, with a rate of 0.001 for the first\n",
    "# 8,000, and 0.0001 for the final 3,000.\n",
    "TRAINING_STEPS = \"12000,3000\"\n",
    "#TRAINING_STEPS = \"120,30\"\n",
    "LEARNING_RATE = \"0.001,0.0001\"\n",
    "\n",
    "# Calculate the total number of steps, which is used to identify the checkpoint\n",
    "# file name.\n",
    "TOTAL_STEPS = str(sum(map(lambda string: int(string), TRAINING_STEPS.split(\",\"))))\n",
    "\n",
    "# Print the configuration to confirm it\n",
    "print(\"Training these words: %s\" % WANTED_WORDS)\n",
    "print(\"Training steps in each stage: %s\" % TRAINING_STEPS)\n",
    "print(\"Learning rate in each stage: %s\" % LEARNING_RATE)\n",
    "print(\"Total number of training steps: %s\" % TOTAL_STEPS)\n",
    "\n",
    "# How long each spectrogram timeslice is.\n",
    "#WINDOW_SIZE_MS = 30 # default: 30\n",
    "# How loud the background noise should be, between 0 and 1.\n",
    "#TRAIN_BACKGROUND_VOLUME_RANGE = 0.1 # default: 0.1\n",
    "# How many of the training samples have background noise mixed in.\n",
    "#TRAIN_BACKGROUND_FREQUENCY = 0.8 # default: 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCgeOpvY9pAi"
   },
   "source": [
    "**DO NOT MODIFY** the following constants as they include filepaths used in this notebook and data that is shared during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nd1iM1o2ymvA"
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of 'silence' and 'unknown' training samples required\n",
    "# to ensure that we have equal number of samples for each label.\n",
    "number_of_labels = WANTED_WORDS.count(',') + 1\n",
    "number_of_total_labels = number_of_labels + 2 # for 'silence' and 'unknown' label\n",
    "equal_percentage_of_training_samples = int(100.0/(number_of_total_labels))\n",
    "SILENT_PERCENTAGE = equal_percentage_of_training_samples\n",
    "UNKNOWN_PERCENTAGE = equal_percentage_of_training_samples\n",
    "\n",
    "# Constants which are shared during training and inference\n",
    "PREPROCESS = 'micro'\n",
    "WINDOW_STRIDE = 20\n",
    "MODEL_ARCHITECTURE = 'tiny_conv' # Other options include: single_fc, conv,\n",
    "                      # low_latency_conv, low_latency_svdf, tiny_embedding_conv\n",
    "\n",
    "# Constants used during training only\n",
    "VERBOSITY = 'WARN'\n",
    "EVAL_STEP_INTERVAL = '1000'\n",
    "SAVE_STEP_INTERVAL = '1000'\n",
    "\n",
    "# Constants for training directories and filepaths\n",
    "DATASET_DIR =  'dataset/'\n",
    "LOGS_DIR = 'logs/'\n",
    "TRAIN_DIR = 'train/' # for training checkpoints and other files.\n",
    "\n",
    "# Constants for inference directories and filepaths\n",
    "import os\n",
    "MODELS_DIR = 'models'\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODEL_TF = os.path.join(MODELS_DIR, 'model.pb')\n",
    "MODEL_TFLITE = os.path.join(MODELS_DIR, 'model.tflite')\n",
    "FLOAT_MODEL_TFLITE = os.path.join(MODELS_DIR, 'float_model.tflite')\n",
    "#MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'model.cc')\n",
    "MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'micro_speech_model_data.cpp')\n",
    "SAVED_MODEL = os.path.join(MODELS_DIR, 'saved_model')\n",
    "\n",
    "QUANT_INPUT_MIN = 0.0\n",
    "QUANT_INPUT_MAX = 26.0\n",
    "QUANT_INPUT_RANGE = QUANT_INPUT_MAX - QUANT_INPUT_MIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rLYpvtg9P4o"
   },
   "source": [
    "## Setup Environment\n",
    "\n",
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ed_XpUrU5DvY"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9Ty5mR58E4i"
   },
   "source": [
    "**DELETE** any old data from previous runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APGx0fEh7hFF"
   },
   "outputs": [],
   "source": [
    "#!rm -rf {DATASET_DIR} {LOGS_DIR} {TRAIN_DIR} {MODELS_DIR}\n",
    "!rm -rf {LOGS_DIR} {TRAIN_DIR} {MODELS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GfEUlfFBizio"
   },
   "source": [
    "Clone the TensorFlow Github Repository, which contains the relevant code required to run this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZArmzT85SLq"
   },
   "outputs": [],
   "source": [
    "#!git clone -q --depth 1 https://github.com/tensorflow/tensorflow\n",
    "#!git clone --quiet --depth 1 --branch v1.15.5 https://github.com/tensorflow/tensorflow\n",
    "#!rm -rf tensorflow\n",
    "if not os.path.exists(\"tensorflow\"):\n",
    "    !git clone --quiet --depth 1 --branch v2.13.0 https://github.com/tensorflow/tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nS9swHLSi7Bi"
   },
   "source": [
    "Load TensorBoard to visualize the accuracy and loss as training proceeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q4qF1VxP3UE4"
   },
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir {LOGS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x1J96Ron-O4R"
   },
   "source": [
    "## Training\n",
    "\n",
    "The following script downloads the dataset and begin training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJsEZx6lynbY",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 17:31:08.644630: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2023-08-30 17:31:08.667597: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3199920000 Hz\n",
      "2023-08-30 17:31:08.671658: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56493b012e90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-30 17:31:08.671685: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "W0830 17:31:39.011559 140481279395648 deprecation.py:506] From /data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/tensorflow/tensorflow/examples/speech_commands/models.py:737: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0830 17:31:39.028212 140481279395648 deprecation.py:323] From /data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0830 18:03:01.010932 140481279395648 deprecation.py:323] From /data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/train.py\n",
    "# https://github.com/tensorflow/tensorflow/tree/r1.15/tensorflow/examples/speech_commands/train.py\n",
    "#!python tensorflow/tensorflow/examples/speech_commands/train.py \\\n",
    "#!python ./tf_train_speech_commands/train.py \\\n",
    "#%run tf_train_speech_commands/train.py \\\n",
    "%run tensorflow/tensorflow/examples/speech_commands/train.py \\\n",
    "#--data_url=  \\\n",
    "--data_dir={DATASET_DIR} \\\n",
    "--wanted_words={WANTED_WORDS} \\\n",
    "--silence_percentage={SILENT_PERCENTAGE} \\\n",
    "--unknown_percentage={UNKNOWN_PERCENTAGE} \\\n",
    "--preprocess={PREPROCESS} \\\n",
    "--window_stride={WINDOW_STRIDE} \\\n",
    "--model_architecture={MODEL_ARCHITECTURE} \\\n",
    "--how_many_training_steps={TRAINING_STEPS} \\\n",
    "--learning_rate={LEARNING_RATE} \\\n",
    "--train_dir={TRAIN_DIR} \\\n",
    "--summaries_dir={LOGS_DIR} \\\n",
    "--verbosity={VERBOSITY} \\\n",
    "--eval_step_interval={EVAL_STEP_INTERVAL} \\\n",
    "--save_step_interval={SAVE_STEP_INTERVAL}\n",
    "#--window_size_ms={WINDOW_SIZE_MS} \\\n",
    "#--background_volume={TRAIN_BACKGROUND_VOLUME_RANGE} \\\n",
    "#--background_frequency={TRAIN_BACKGROUND_FREQUENCY}\n",
    "# added: window_size_ms background_volume background_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UczQKtqLi7OJ"
   },
   "source": [
    "# Skipping the training\n",
    "\n",
    "If you don't want to spend an hour or two training the model from scratch, you can download pretrained checkpoints by uncommenting the lines below (removing the '#'s at the start of each line) and running them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RZw3VNlnla-J"
   },
   "outputs": [],
   "source": [
    "#!curl -O \"https://storage.googleapis.com/download.tensorflow.org/models/tflite/speech_micro_train_2020_05_10.tgz\"\n",
    "#!tar xzf speech_micro_train_2020_05_10.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQUJLrdS-ftl"
   },
   "source": [
    "## Generate a TensorFlow Model for Inference\n",
    "\n",
    "Combine relevant training results (graph, weights, etc) into a single file for inference. This process is known as freezing a model and the resulting model is known as a frozen model/graph, as it cannot be further re-trained after this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redirect output to file\n",
    "!rm -f jupyter_freeze_log.txt\n",
    "import sys\n",
    "sys.stdout = open(\"jupyter_freeze_log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xyc3_eLh9sAg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0831 13:40:41.424189 140481279395648 deprecation.py:323] From /data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/tensorflow/tensorflow/examples/speech_commands/freeze.py:183: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "!rm -rf {SAVED_MODEL}\n",
    "#!python tensorflow/tensorflow/examples/speech_commands/freeze.py \\\n",
    "#!python tf_train_speech_commands/freeze.py \\\n",
    "%run tensorflow/tensorflow/examples/speech_commands/freeze.py \\\n",
    "--wanted_words=$WANTED_WORDS \\\n",
    "--window_stride_ms=$WINDOW_STRIDE \\\n",
    "--preprocess=$PREPROCESS \\\n",
    "--model_architecture=$MODEL_ARCHITECTURE \\\n",
    "--start_checkpoint=$TRAIN_DIR$MODEL_ARCHITECTURE'.ckpt-'{TOTAL_STEPS} \\\n",
    "--save_format=saved_model \\\n",
    "--output_file={SAVED_MODEL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DBGDxVI-nKG"
   },
   "source": [
    "## Generate a TensorFlow Lite Model\n",
    "\n",
    "Convert the frozen graph into a TensorFlow Lite model, which is fully quantized for use with embedded devices.\n",
    "\n",
    "The following cell will also print the model size, which will be under 20 kilobytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redirect output to file\n",
    "!rm -f jupyter_model_log.txt\n",
    "import sys\n",
    "sys.stdout = open(\"jupyter_model_log.txt\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIitkqvGWmre"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# We add this path so we can import the speech processing modules.\n",
    "#sys.path.append(\"/content/tensorflow/tensorflow/examples/speech_commands/\")\n",
    "import input_data\n",
    "import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzqECqMxgBh4"
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "CLIP_DURATION_MS = 1000\n",
    "WINDOW_SIZE_MS = 30.0\n",
    "FEATURE_BIN_COUNT = 40\n",
    "BACKGROUND_FREQUENCY = 0.8\n",
    "BACKGROUND_VOLUME_RANGE = 0.1\n",
    "#BACKGROUND_FREQUENCY = TRAIN_BACKGROUND_FREQUENCY\n",
    "#BACKGROUND_VOLUME_RANGE = TRAIN_BACKGROUND_VOLUME_RANGE\n",
    "TIME_SHIFT_MS = 100.0\n",
    "\n",
    "#DATA_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
    "DATA_URL=''\n",
    "VALIDATION_PERCENTAGE = 10\n",
    "TESTING_PERCENTAGE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rNQdAplJV1fz"
   },
   "outputs": [],
   "source": [
    "model_settings = models.prepare_model_settings(\n",
    "    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),\n",
    "    SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
    "    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
    "audio_processor = input_data.AudioProcessor(\n",
    "    DATA_URL, DATASET_DIR,\n",
    "    SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,\n",
    "    WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,\n",
    "    TESTING_PERCENTAGE, model_settings, LOGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBj_AyCh1cC0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 13:48:23.834559: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\n",
      "2023-08-31 13:48:23.834662: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2023-08-31 13:48:23.838550: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\n",
      "2023-08-31 13:48:23.838562: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "2023-08-31 13:48:23.838566: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "2023-08-31 13:48:23.847662: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\n",
      "2023-08-31 13:48:23.847976: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2023-08-31 13:48:23.850650: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node first_weights/Assign doesn't exist in graph\n",
      "2023-08-31 13:48:24.742062: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\n",
      "2023-08-31 13:48:24.742181: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2023-08-31 13:48:24.746629: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\n",
      "2023-08-31 13:48:24.746642: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "2023-08-31 13:48:24.746646: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "2023-08-31 13:48:24.756424: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\n",
      "2023-08-31 13:48:24.756731: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2023-08-31 13:48:24.759420: E tensorflow/core/grappler/grappler_item_builder.cc:656] Init node first_weights/Assign doesn't exist in graph\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'data/time_shift_offset' with dtype int32 and shape [2]\n\t [[node data/time_shift_offset (defined at data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'data/time_shift_offset':\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n    app.start()\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 387, in do_execute\n    cell_id=cell_id,\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2976, in run_cell\n    raw_cell, store_history, silent, shell_futures, cell_id\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3258, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"tmp/ipykernel_1742138/1240538038.py\", line 9, in <module>\n    TESTING_PERCENTAGE, model_settings, LOGS_DIR)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/tensorflow/tensorflow/examples/speech_commands/input_data.py\", line 201, in __init__\n    \"\"\"Download and extract data set tar file.\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/tensorflow/tensorflow/examples/speech_commands/input_data.py\", line 409, in prepare_processing_graph\n    paddings=self.time_shift_padding_placeholder_,\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 2619, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 6669, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'data/time_shift_offset' with dtype int32 and shape [2]\n\t [[{{node data/time_shift_offset}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1742138/2607024760.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mflattened_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentative_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepresentative_dataset_gen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mtflite_model_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_TFLITE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Quantized model is %d bytes\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtflite_model_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_calibration_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       result = self._calibrate_quantize_model(result, inference_input_type,\n\u001b[0;32m--> 993\u001b[0;31m                                               inference_output_type)\n\u001b[0m\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\u001b[0m in \u001b[0;36m_calibrate_quantize_model\u001b[0;34m(self, result, inference_input_type, inference_output_type)\u001b[0m\n\u001b[1;32m    237\u001b[0m     return calibrate_quantize.calibrate_and_quantize(\n\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentative_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_input_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         inference_output_type, allow_float)\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_base_converter_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/lite/python/optimize/calibrator.py\u001b[0m in \u001b[0;36mcalibrate_and_quantize\u001b[0;34m(self, dataset_gen, input_type, output_type, allow_float)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \"\"\"\n\u001b[1;32m     73\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mcalibration_sample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeedTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalibration_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     return self._calibrator.QuantizeModel(\n",
      "\u001b[0;32m/tmp/ipykernel_1742138/2607024760.py\u001b[0m in \u001b[0;36mrepresentative_dataset_gen\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                                          \u001b[0mTIME_SHIFT_MS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                          \u001b[0;34m'testing'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                          sess)\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mflattened_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1960\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mflattened_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/tensorflow/tensorflow/examples/speech_commands/input_data.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, how_many, offset, model_settings, background_frequency, background_volume_range, time_shift, mode, sess)\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m       \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m       \u001b[0mlabel_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m       \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'data/time_shift_offset' with dtype int32 and shape [2]\n\t [[node data/time_shift_offset (defined at data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'data/time_shift_offset':\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n    app.start()\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 387, in do_execute\n    cell_id=cell_id,\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2976, in run_cell\n    raw_cell, store_history, silent, shell_futures, cell_id\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3258, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"tmp/ipykernel_1742138/1240538038.py\", line 9, in <module>\n    TESTING_PERCENTAGE, model_settings, LOGS_DIR)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/tensorflow/tensorflow/examples/speech_commands/input_data.py\", line 201, in __init__\n    \"\"\"Download and extract data set tar file.\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/tensorflow/tensorflow/examples/speech_commands/input_data.py\", line 409, in prepare_processing_graph\n    paddings=self.time_shift_padding_placeholder_,\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\", line 2619, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 6669, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"data_fast/storage/cstollen/vedliot/projects/secure-microphone/training/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow 1\n",
    "#with tf.Session() as sess:\n",
    "with tf.compat.v1.Session() as sess:\n",
    "  float_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
    "  float_tflite_model = float_converter.convert()\n",
    "  float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
    "  print(\"Float model is %d bytes\" % float_tflite_model_size)\n",
    "\n",
    "  converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  converter.inference_input_type = tf.lite.constants.INT8\n",
    "  converter.inference_output_type = tf.lite.constants.INT8\n",
    "  def representative_dataset_gen():\n",
    "    for i in range(100):\n",
    "      data, _ = audio_processor.get_data(1, i*1, model_settings,\n",
    "                                         BACKGROUND_FREQUENCY, \n",
    "                                         BACKGROUND_VOLUME_RANGE,\n",
    "                                         TIME_SHIFT_MS,\n",
    "                                         'testing',\n",
    "                                         sess)\n",
    "      flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(1, 1960)\n",
    "      yield [flattened_data]\n",
    "  converter.representative_dataset = representative_dataset_gen\n",
    "  tflite_model = converter.convert()\n",
    "  tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
    "  print(\"Quantized model is %d bytes\" % tflite_model_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lBj_AyCh1cC0"
   },
   "outputs": [],
   "source": [
    "# Tensorflow 2\n",
    "if False:\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "    # with tf.Session() as sess:\n",
    "      float_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
    "      float_tflite_model = float_converter.convert()\n",
    "      float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
    "      print(\"Float model is %d bytes\" % float_tflite_model_size)\n",
    "\n",
    "      converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
    "      converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "      #converter.inference_input_type = tf.lite.constants.INT8\n",
    "      #converter.inference_output_type = tf.lite.constants.INT8\n",
    "\n",
    "      # TF 2\n",
    "      converter.target_spec.supported_types = [tf.int8]\n",
    "\n",
    "      def representative_dataset_gen():\n",
    "        for i in range(100):\n",
    "          data, _ = audio_processor.get_data(1, i*1, model_settings,\n",
    "                                             BACKGROUND_FREQUENCY, \n",
    "                                             BACKGROUND_VOLUME_RANGE,\n",
    "                                             TIME_SHIFT_MS,\n",
    "                                             'testing',\n",
    "                                             sess)\n",
    "          flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(1, 1960)\n",
    "          yield [flattened_data]\n",
    "      converter.representative_dataset = representative_dataset_gen\n",
    "      tflite_model = converter.convert()\n",
    "      tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
    "      print(\"Quantized model is %d bytes\" % tflite_model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EeLiDZTbLkzv"
   },
   "source": [
    "## Testing the TensorFlow Lite model's accuracy\n",
    "\n",
    "Verify that the model we've exported is still accurate, using the TF Lite Python API and our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQsEteKRLryJ"
   },
   "outputs": [],
   "source": [
    "# Tensorflow 1\n",
    "# Helper function to run inference\n",
    "def run_tflite_inference(tflite_model_path, model_type=\"Float\"):\n",
    "  # Load test data\n",
    "  np.random.seed(0) # set random seed for reproducible test results.\n",
    "\n",
    "  tf.compat.v1.disable_eager_execution()\n",
    "  with tf.compat.v1.Session() as sess:\n",
    "    \n",
    "  # with tf.Session() as sess:\n",
    "    test_data, test_labels = audio_processor.get_data(\n",
    "        -1, 0, model_settings, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n",
    "        TIME_SHIFT_MS, 'testing', sess)\n",
    "  test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  # For quantized models, manually quantize the input data from float to integer\n",
    "  if model_type == \"Quantized\":\n",
    "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "    test_data = test_data / input_scale + input_zero_point\n",
    "    test_data = test_data.astype(input_details[\"dtype\"])\n",
    "\n",
    "  correct_predictions = 0\n",
    "  for i in range(len(test_data)):\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    top_prediction = output.argmax()\n",
    "    correct_predictions += (top_prediction == test_labels[i])\n",
    "\n",
    "  print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
    "      model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQsEteKRLryJ"
   },
   "outputs": [],
   "source": [
    "# Tensorflow 2\n",
    "if False:\n",
    "    # Helper function to run inference\n",
    "    def run_tflite_inference(tflite_model_path, model_type=\"Float\"):\n",
    "      # Load test data\n",
    "      np.random.seed(0) # set random seed for reproducible test results.\n",
    "      with tf.Session() as sess:\n",
    "        test_data, test_labels = audio_processor.get_data(\n",
    "            -1, 0, model_settings, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n",
    "            TIME_SHIFT_MS, 'testing', sess)\n",
    "      test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n",
    "\n",
    "      # Initialize the interpreter\n",
    "      interpreter = tf.lite.Interpreter(tflite_model_path)\n",
    "      interpreter.allocate_tensors()\n",
    "\n",
    "      input_details = interpreter.get_input_details()[0]\n",
    "      output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "      # For quantized models, manually quantize the input data from float to integer\n",
    "      if model_type == \"Quantized\":\n",
    "        input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "        test_data = test_data / input_scale + input_zero_point\n",
    "        test_data = test_data.astype(input_details[\"dtype\"])\n",
    "\n",
    "      correct_predictions = 0\n",
    "      for i in range(len(test_data)):\n",
    "        interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "        top_prediction = output.argmax()\n",
    "        correct_predictions += (top_prediction == test_labels[i])\n",
    "\n",
    "      print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
    "          model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l-pD52Na6jRa"
   },
   "outputs": [],
   "source": [
    "# Compute float model accuracy\n",
    "run_tflite_inference(FLOAT_MODEL_TFLITE)\n",
    "\n",
    "# Compute quantized model accuracy\n",
    "run_tflite_inference(MODEL_TFLITE, model_type='Quantized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dt6Zqbxu-wIi"
   },
   "source": [
    "## Generate a TensorFlow Lite for MicroControllers Model\n",
    "Convert the TensorFlow Lite model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XohZOTjR8ZyE"
   },
   "outputs": [],
   "source": [
    "# Install xxd if it is not available\n",
    "#!apt-get update && apt-get -qq install xxd\n",
    "# Convert to a C source file\n",
    "#!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
    "# Update variable names\n",
    "#REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
    "#!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XohZOTjR8ZyE"
   },
   "outputs": [],
   "source": [
    "# Install xxd if it is not available\n",
    "#!apt-get update && apt-get -qq install xxd\n",
    "# Convert to a C source file\n",
    "#!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
    "#MICRO_MODEL_FILENAME = \"models/micro_speech_model_data.cpp\"\n",
    "#print(MODEL_TFLITE)\n",
    "#print(MODEL_TFLITE_MICRO)\n",
    "#print(MICRO_MODEL_FILENAME)\n",
    "#!echo {MODEL_TFLITE} {MICRO_MODEL_FILENAME}\n",
    "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
    "# Update variable names\n",
    "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
    "#print(REPLACE_TEXT)\n",
    "!sed -i 's/_len/_size/g' {MODEL_TFLITE_MICRO}\n",
    "!sed -i 's/unsigned/const unsigned/g' {MODEL_TFLITE_MICRO}\n",
    "#!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}\n",
    "!sed -i 's/'{REPLACE_TEXT}'/g_micro_speech_model_data/g' {MODEL_TFLITE_MICRO}\n",
    "#!sed -i '1s/^/#include <cstdint>\\n#include \"micro_speech_model_data.h\"\\nconst unsigned int g_micro_speech_model_data_size = '{tflite_model_size}';\\nalignas(16) const /g' {MODEL_TFLITE_MICRO}\n",
    "!sed -i '1s/^/#include <cstdint>\\n#include \"micro_speech_model_data.h\"\\nalignas(16) /g' {MODEL_TFLITE_MICRO}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pQnN0i_-0L2"
   },
   "source": [
    "## Deploy to a Microcontroller\n",
    "\n",
    "Follow the instructions in the [micro_speech](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech) README.md for [TensorFlow Lite for MicroControllers](https://www.tensorflow.org/lite/microcontrollers/overview) to deploy this model on a specific microcontroller.\n",
    "\n",
    "**Reference Model:** If you have not modified this notebook, you can follow the instructions as is, to deploy the model. Refer to the [`micro_speech/train/models`](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/train/models) directory to access the models generated in this notebook.\n",
    "\n",
    "**New Model:** If you have generated a new model to identify different words: (i) Update `kCategoryCount` and `kCategoryLabels` in [`micro_speech/micro_features/micro_model_settings.h`](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/micro_features/micro_model_settings.h) and (ii) Update the values assigned to the variables defined in [`micro_speech/micro_features/model.cc`](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/micro_features/model.cc) with values displayed after running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoYyh0VU8pca"
   },
   "outputs": [],
   "source": [
    "# Redirect output to file\n",
    "#!rm -f micro_speech_model_data.cpp\n",
    "#import sys\n",
    "#sys.stdout = open(\"micro_speech_model_data.cpp\", \"w\")# Print the C source file\n",
    "#!cat {MODEL_TFLITE_MICRO}\n",
    "#!cat {MICRO_MODEL_FILENAME}\n",
    "#print(\"MODEL_TFLITE_MICRO: \", MODEL_TFLITE_MICRO)\n",
    "#print(\"MICRO_MODEL_FILENAME: \", MICRO_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print log\n",
    "#!cat jupyter_model_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_micro_speech_model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
